{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Project 3: Projection Matrix and Fundamental Matrix Estimation with RANSAC](https://dellaert.github.io/19F-4476/proj3.html/)\n",
    "\n",
    "\n",
    "(1) Projection Matrix  \n",
    "(2) Fundamental Matrix Estimation  \n",
    "(3) Fundamental Matrix with RANSAC  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from proj3_code.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 Projection Matrix Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.1 Implement Camera Projection\n",
    "\n",
    "In this initial part, in `projection_matrix.py` you will implement camera projection in the `projection(P, points_3d)` from homogenous world coordinates $X_i = [X_i, Y_i, Z_i, 1]$ to non-homogenous pixel coordinates $x_i, y_i$.\n",
    "\n",
    "It will be helpful to recall the equations to convert to pixel coordinates\n",
    "\n",
    "\\begin{align}\n",
    "x_i = \\frac{p_{11}X_i+p_{12}Y_i + p_{13}Z_i + p_{14}}{p_{31}X_i+p_{32}Y_i + p_{33}Z_i + p_{34}} \\quad y_i = \\frac{p_{21}X_i+p_{22}Y_i + p_{23}Z_i + p_{24}}{p_{31}X_i+p_{32}Y_i + p_{33}Z_i + p_{34}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import projection_matrix\n",
    "# np.set_printoptions(suppress=True) # Suppresses printing in scientific notation\n",
    "\n",
    "from unit_tests.part1_unit_test import (\n",
    "    verify, \n",
    "    test_projection, \n",
    "    test_objective_func,\n",
    "    test_decompose_camera_matrix,\n",
    "    test_calculate_camera_center,\n",
    "    test_estimate_camera_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for camera projection:', verify(test_projection))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.2: Objective Function \n",
    "\n",
    "In this part, in `projection_matrix.py` you will implement the objective function `objective_function(x, **kwargs)` that will be passed to `scipy.optimize.least_squares` for minimization with the Levenberg-Marquardt algorithm. The input to this function is a [vectorized](https://en.wikipedia.org/wiki/Vectorization_(mathematics)) camera matrix, the output is the term that gets squared in the objective function and should also be vectorized. Scipy takes care of the squaring + summation part.\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_{i=1}^N ( \\color{purple}{\\hat{\\mathbf{P}}\\mathbf{X}_w^i-\\mathbf{x}^i })^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for objective_function:', verify(test_objective_func))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.3: Estimating the Projection Matrix Given Point Correspondences\n",
    "\n",
    "Initially you will run the optimization to estimate $\\mathbf{P}$ using an initial guess that we provide.\n",
    "\n",
    "### Good initial estimate for $\\mathbf{P}$.\n",
    "\n",
    "Optimizing the reprojection loss using Levenberg-Marquardt requires a good initial estimate for $\\mathbf{P}$. This can be done by having good initial estimates for $\\mathbf{K}$ and $\\mathbf{R}^T$ and $\\mathbf{t}$ which you can multiply to then generate your estimated $\\mathbf{K}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_K = np.array([[ 500,   0, 535],\n",
    "                            [   0, 500, 390],\n",
    "                            [   0,   0,  -1]])\n",
    "\n",
    "initial_guess_R_T = np.array([[ 0.5,   -1,  0],\n",
    "                            [   0,    0, -1],\n",
    "                            [   1,  0.5,  0]])\n",
    "\n",
    "initial_guess_I_t = np.array([[   1,    0, 0, 300],\n",
    "                              [   0,    1, 0, 300],\n",
    "                              [   0,    0, 1,  30]])\n",
    "\n",
    "initial_guess_P = np.matmul(initial_guess_K, np.matmul(initial_guess_R_T, initial_guess_I_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-69749c37c350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg_path\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/pic_b.jpg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpoints_2d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts2d_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpoints_3d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts3d_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# set the paths and load the data\n",
    "pts2d_path = '../data/pts2d-pic_b.txt'\n",
    "pts3d_path = '../data/pts3d.txt'\n",
    "img_path   = '../data/pic_b.jpg'\n",
    "\n",
    "points_2d = np.loadtxt(pts2d_path)\n",
    "points_3d = np.loadtxt(pts3d_path)\n",
    "img = load_image(img_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate the projection matrix given corresponding 2D & 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = projection_matrix.estimate_camera_matrix(points_2d, points_3d, initial_guess_P)\n",
    "\n",
    "print('The projection matrix is\\n', P)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(P, points_2d, points_3d);\n",
    "\n",
    "# residual is the sum of Euclidean distances between actual and projected points\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "visualize_points_image(points_2d, projected_2d_pts, '../data/pic_b.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for estimate_camera_matrix:', verify(test_estimate_camera_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.4: Decomposing the projection matrix\n",
    "\n",
    "In this part in `projection_matrix.py` you will implement `decompose_camera_matrix(P)` that takes as input the camera matrix $\\mathbf{P}$ and outputs the intrinsic $\\mathbf{K}$ and rotation matrix ${}_c\\mathbf{R}_w$, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for decomposing projection matrix:', verify(test_decompose_camera_matrix))\n",
    "K, R = projection_matrix.decompose_camera_matrix(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.5: Calculating Camera Center\n",
    "\n",
    "In this part in `projection_matrix.py` you will implement `calculate_camera_center(P, K, R)` that takes as input the \n",
    "projection $\\mathbf{P}$, intrinsic $\\mathbf{K}$ and extrinsic ${}_c\\mathbf{R}_w$ matrix and outputs the camera position in world coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test for calculating camera center:', verify(test_calculate_camera_center))\n",
    "center = projection_matrix.calculate_camera_center(P, K, R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the camera center and the camera coordinate system as well as the  the world coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3dview(points_3d, center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.6: Taking Your Own Images and Estimating the Projection Matrix + Camera Pose\n",
    "\n",
    "In this part you will take two images of your fiducial object. If you want to also reuse these images for Part II, keep in mind how to take good images for estimating the Fundamental Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image1_path = '../data/book_img1.jpg'\n",
    "image2_path = '../data/book_img2.jpg'\n",
    "\n",
    "img1 = load_image(image1_path)\n",
    "img2 = load_image(image2_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure your fiducial object and define a coordinate system. Fill out the `points_3d` variable with the 3D point locations of the points you'll use for correspondences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_3d = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each image, find the 2D pixel locations of your 3D points. Hovering over the image gives you the `x,y` coordinates of your cursor on the image. You can use the lower left side controls to zoom into the image for more precise measurements. Fill out `points2d_img1` with these coordinate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting image 1\n",
    "fig = plt.figure(); ax = fig.add_subplot(111); ax.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2d_img1 = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting image 2\n",
    "fig = plt.figure(); ax = fig.add_subplot(111); ax.imshow(img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points2d_img2 = np.array([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our objective function will need to read the measurements you just saved from disk. We need to save this data now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('../data/pts3d_fiducial.npy', points_3d)\n",
    "np.savetxt('../data/pts2d_image1.npy', points2d_img1)\n",
    "np.savetxt('../data/pts2d_image2.npy', points2d_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.7: Making your own $\\mathbf{K}$,  $\\mathbf{R}^T$ and $[\\mathbf{I}|\\mathbf{t}]$ estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_K = np.array([[   1,   0,  0],\n",
    "                            [   0,   1,  0],\n",
    "                            [   0,   0,  1]])\n",
    "\n",
    "initial_guess_R_T = np.array([[ 1,   0, 0],\n",
    "                              [ 0,   1, 0],\n",
    "                              [ 0,   0, 1]])\n",
    "\n",
    "initial_guess_I_t = np.array([[1, 0, 0, 0],\n",
    "                              [0, 1, 0, 0],\n",
    "                              [0, 0, 1, 0]])\n",
    "\n",
    "initial_guess_P = np.matmul(initial_guess_K, np.matmul(initial_guess_R_T, initial_guess_I_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the paths and load the data\n",
    "pts2d_path = '../data/pts2d_image1.npy'\n",
    "pts3d_path = '../data/pts3d_fiducial.npy'\n",
    "\n",
    "points_2d = np.loadtxt(pts2d_path)\n",
    "points_3d = np.loadtxt(pts3d_path)\n",
    "img = load_image(image1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your estimate for the camera pose relative to the world coordinate system. RGB colors correspond with XYZ (first, second and third coordinate). Be mindful of whether you should be passing `R` or `R.T` in for your rotation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3dview_with_coordinates(points_3d, initial_guess_I_t[:,3], initial_guess_R_T.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the optimization results given your initial guess. If your initial guess is poor the optimizition **will not** work. You will need to make initial estimates for both the images you took."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = projection_matrix.estimate_camera_matrix(points_2d, points_3d, initial_guess_P)\n",
    "print('The projection matrix is\\n', P1)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(P1, points_2d, points_3d);\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "visualize_points_image(points_2d, projected_2d_pts, image1_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your estimate for the camera pose relative to the world coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the pats and load the data\n",
    "pts2d_path = '../data/pts2d_image2.npy'\n",
    "pts3d_path = '../data/pts3d_fiducial.npy'\n",
    "\n",
    "points_2d = np.loadtxt(pts2d_path)\n",
    "points_3d = np.loadtxt(pts3d_path)\n",
    "img = load_image(image1_path)\n",
    "np.array(img.shape[:2])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_guess_K = np.array([[   1,   0,  0],\n",
    "                            [   0,   1,  0],\n",
    "                            [   0,   0,  1]])\n",
    "\n",
    "initial_guess_R_T = np.array([[ 1,   0, 0],\n",
    "                              [ 0,   1, 0],\n",
    "                              [ 0,   0, 1]])\n",
    "\n",
    "initial_guess_I_t = np.array([[1, 0, 0, 0],\n",
    "                              [0, 1, 0, 0],\n",
    "                              [0, 0, 1, 0]])\n",
    "\n",
    "initial_guess_P = np.matmul(initial_guess_K, np.matmul(initial_guess_R_T, initial_guess_I_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot3dview_with_coordinates(points_3d, initial_guess_I_t[:,3], initial_guess_R_T.T) #change this plot to show the world coordinate system better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P2 = projection_matrix.estimate_camera_matrix(points_2d, points_3d, initial_guess_P)\n",
    "#M = sc.calculate_projection_matrix(points_2d, points_3d)\n",
    "print('The projection matrix is\\n', P2)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(P2, points_2d, points_3d);\n",
    "print('The total residual is {:f}'.format(residual))\n",
    "visualize_points_image(points_2d, projected_2d_pts, image2_path)\n",
    "#visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1.8 Visualizing both camera poses in the world coordinate system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K1, R1 = projection_matrix.decompose_camera_matrix(P1)\n",
    "center_1 = projection_matrix.calculate_camera_center(P1, K1, R1);\n",
    "print(center_1)\n",
    "\n",
    "K2, R2 = projection_matrix.decompose_camera_matrix(P2)\n",
    "center_2 = projection_matrix.calculate_camera_center(P2, K2, R2);\n",
    "print(center_2)\n",
    "\n",
    "plot3dview_2_cameras(points_3d, center_1, center_2, R1.T, R2.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 Fundamental Matrix Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you'll be estimating the fundamental matrix. You'll be using a least squares optimizer from SciPy. (Documentation here: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html)\n",
    "\n",
    "The least squares optimizer takes an objective function, your variables to optimize, and the points that you want to fit a line to. In this case, the objective function is to minimize the point to line distance, where the line is the projection of a point onto another image by the fundamental matrix, and the point is an actual point of a feature in that image. The variable that you want to optimize would be the 9 values in the 3x3 Fundamental Matrix. The points that you are optimizing over are provided to you, and they are the homogeneous coordinates of corresponding features from two different images of the same scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1 Estimate Fundamental Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `point_line_distance()` method in fundamental_matrix.py.\n",
    "\n",
    "\\begin{align}\n",
    "    d(line, point) = \\frac{au + bv + c}{\\sqrt{a^2 + b^2}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unit_tests.test_fundamental_matrix import verify\n",
    "from unit_tests.test_fundamental_matrix import TestFundamentalMatrix\n",
    "\n",
    "test_fundamental_matrix_stereo = TestFundamentalMatrix()\n",
    "TestFundamentalMatrix.setUp(test_fundamental_matrix_stereo)\n",
    "print(\"test_point_line_distance(): \" + verify(test_fundamental_matrix_stereo.test_point_line_distance))\n",
    "print(\"test_point_line_distance_zero(): \" + verify(test_fundamental_matrix_stereo.test_point_line_distance_zero))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement `signed_point_line_errors()`.\n",
    "\n",
    "Keep in mind that SciPy does the squaring and summing for you, so all you have to do in `signed_point_line_errors()` is return a list of each individual error. So if there are 9 points, you have to calculate the `point_line_distance()` between each pair from $Fx_1$ to $x_0$ and also $F^Tx_0$ to $x_1$, then append all errors to a list, such that you end up returning a list of length 18. SciPy will take the list and square each element and sum everything for you. The red parts in the equation below are the parts you'll need to implement.\n",
    "\n",
    "`signed_point_line_errors()`:\n",
    "\\begin{align}\n",
    "    \\color{red}{d(Fx_1, x_0)}^2 + \\color{red}{d(F^T x_0, x_1)}^2\n",
    "\\end{align}\n",
    "\n",
    "You'll also have to make the call to SciPy's least squares optimizer in the `optimize()` method in least_squares_fundamental_matrix.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unit_tests.test_fundamental_matrix import TestFundamentalMatrix2, TestFundamentalMatrix3\n",
    "\n",
    "print(\"TestFundamentalMatrix():\")\n",
    "print(\"test_signed_point_line_errors(): \" + verify(test_fundamental_matrix_stereo.test_signed_point_line_errors))\n",
    "print(\"test_least_squares_optimize(): \" + verify(test_fundamental_matrix_stereo.test_least_squares_optimize))\n",
    "\n",
    "print(\"TestFundamentalMatrix2():\")\n",
    "test_fundamental_matrix_synthetic = TestFundamentalMatrix2()\n",
    "TestFundamentalMatrix2.setUp(test_fundamental_matrix_synthetic)\n",
    "print(\"test_signed_point_line_errors(): \" + verify(test_fundamental_matrix_synthetic.test_signed_point_line_errors))\n",
    "print(\"test_least_squares_optimize(): \" + verify(test_fundamental_matrix_synthetic.test_least_squares_optimize))\n",
    "\n",
    "print(\"TestFundamentalMatrix3():\")\n",
    "test_fundamental_matrix_real = TestFundamentalMatrix3()\n",
    "TestFundamentalMatrix3.setUp(test_fundamental_matrix_real)\n",
    "print(\"test_signed_point_line_errors(): \" + verify(test_fundamental_matrix_real.test_signed_point_line_errors))\n",
    "print(\"test_least_squares_optimize(): \" + verify(test_fundamental_matrix_real.test_least_squares_optimize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following cell to find the Fundamental Matrix using least squares. You should see the epipolar lines in the correct places in the image. **You'll need to screenshot this and put it in your report.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Load the data for room images\n",
    "points_2d_pic_a = np.loadtxt('../data/pts2d-pic_a.txt')\n",
    "points_2d_pic_b = np.loadtxt('../data/pts2d-pic_b.txt')\n",
    "img_left = load_image('../data/pic_a.jpg')\n",
    "img_right = load_image('../data/pic_b.jpg')\n",
    "\n",
    "import least_squares_fundamental_matrix as ls\n",
    "\n",
    "F = ls.solve_F(points_2d_pic_a, points_2d_pic_b)\n",
    "print(F)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(F, img_left, img_right, points_2d_pic_a, points_2d_pic_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Try Fundamental Matrix Estimation Yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're going to take two images yourself and estimate the fundamental matrix between them. To do this, take two images and save them as \"my_image_0.jpg\" and \"my_image_1.jpg\" in the \"/data\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data for room images\n",
    "my_img_left = load_image('../data/my_image_0.jpg')\n",
    "my_img_right = load_image('../data/my_image_1.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To collect your own data points, run the following cell and mouse over features in the image and record the x- and y-coordinates. You'll need at least 9 points because we are trying to optimize for 9 variables, one for each element in the 3x3 fundamental matrix. Think about how you can choose good features for estimating the fundamental matrix.\n",
    "\n",
    "**Store your points in variable \"my_image_0_pts\" and \"my_image_1_pts\" respectively.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib\n",
    "# plotting image 1\n",
    "image_0 = plt.figure(); image_0_ax = image_0.add_subplot(111); image_0_ax.imshow(my_img_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_0_pts = np.array([[0, 0]]) # Record your coordinates here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting image 2\n",
    "image_1 = plt.figure(); image_1_ax = image_1.add_subplot(111); image_1_ax.imshow(my_img_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_1_pts = np.array([[0, 0]]) # Record your coordinates here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import two_view_data as two_view_data\n",
    "my_F = ls.solve_F(my_image_0_pts, my_image_1_pts)\n",
    "print(my_F)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(my_F, my_img_left, my_img_right, my_image_0_pts, my_image_1_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fundamental Matrix with RANSAC (Szeliski 6.1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will automate the process of finding the fundamental matrix for an image by using SIFTNet to identify interest points and using RANSAC to robustly find true interest point matches between the two images. We will give you a correct implementation of SIFTNet.\n",
    "\n",
    "We will implement a workflow using the SIFTNet from project 2\n",
    "to extract feature points, then RANSAC will select a random subset of those points, \n",
    "you will call your function from part 2 to calculate the fundamental matrix for those points, \n",
    "and then you will check how many other points identified by SIFTNet match this\n",
    "fundamental matrix. Then you will repeat this process and select another subset of points using RANSAC until you find the subset of points that produces the best fundamental matrix with the most\n",
    "matching points. Refer to the lecture slides for the RANSAC workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find a simple explanation of RANSAC at \n",
    "https://www.mathworks.com/discovery/ransac.html.\n",
    "See section 6.1.4 in the textbook for a more thorough explanation of how RANSAC works.\n",
    "\n",
    "### RANSAC Iterations\n",
    "Begin by calculating the number of iterations $S$ RANSAC will need to perform to guarentee a given success rate $P$ knowing the number of points included in the sample $k$ and the probability of an individual point being a true match $p$. To derive this formula, consider the following:\n",
    " * the probability that any one point has a true match is $p$\n",
    "  * conversely the probability that any one point is not a match is $1-p$\n",
    "  * the probability that two points are both matches is then $p \\cdot p$\n",
    "  * this can be extendeed to $k$ points, for which the probability that they are all true matches is $p^k$\n",
    " * on the other hand, we want the probability that $k$ points are all true matches to be $P$ (and the probability that they are not to be $1-P$)\n",
    " * by repeatedly sampling $k$ points, we can reduce the probability that all of the samples do not contain $k$ true matches\n",
    " * After $S$ samples we want the probability of failure to equal $1-P$\n",
    " \n",
    "Start by setting up this equality $$1-P = ...$$\n",
    "and then rearange it to write a function to solve for $S$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from ransac import calculate_num_ransac_iterations\n",
    "from unit_tests.test_ransac import test_calculate_num_ransac_iterations\n",
    "\n",
    "P = 0.999\n",
    "k = 9\n",
    "p = 0.90\n",
    "# call their ransac iterations function\n",
    "S = calculate_num_ransac_iterations(P, k, p)\n",
    "# print number of trials they will need to run\n",
    "print('S =', int(S))\n",
    "\n",
    "print(\"Test for calculate_num_ransac_iterations(): \" + verify(test_calculate_num_ransac_iterations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "*put these answers in your report*\n",
    "\n",
    "How many RANSAC iterations would we need to find the fundamental matrix with 99.9% certainty from your Mount Rushmore SIFTNet results assuming that they had a 90% point correspondence accuracy?\n",
    "\n",
    "One might imagine that if we had more than 9 point correspondences, it would be better to use more of them to solve for the fundamental matrix. Investigate this by finding the number of RANSAC iterations you would need to run with 18 points.\n",
    "\n",
    "If our dataset had a lower point correspondence accuracy, say 70%, what is the minimum number of iterations needed to find the fundamental matrix with 99.9% certainty?\n",
    "\n",
    "At the end of this assignment you will be performing RANSAC to find the fundamental matrix for an image pair that you create, and you will want to keep the estimated accuracy of your point correspondences in mind when deciding how many iterations are appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANSAC Implementation\n",
    "Next we will implement the RANSAC algorithm. Remember the steps from the link above:\n",
    " 1. Randomly selecting a subset of the data set\n",
    " 1. Fitting a model to the selected subset\n",
    " 1. Determining the number of outliers\n",
    " 1. Repeating steps 1-3 for a prescribed number of iterations\n",
    "\n",
    "For the application of finding true point pair matches and using them to calculate the fundamental matrix, our subset of the data will be the minimum number of point pairs needed to calculate the fundamental matrix.\n",
    "The model we are fitting is the fundamental matrix.\n",
    "Outliers will be found by using the `point_line_distance()` error function from part 2 and thresholding with a certain margin of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ransac import ransac_fundamental_matrix\n",
    "from unit_tests import test_ransac\n",
    "\n",
    "points_a = np.load('../unit_tests/pointsa.npy')\n",
    "points_b = points_a\n",
    "\n",
    "F, _, _ = ransac_fundamental_matrix(points_a, points_b)\n",
    "print('F= ', F)\n",
    "\n",
    "print(\"Test for ransac_find_inliers(): \" + verify(test_ransac.test_ransac_find_inliers))\n",
    "print(\"Test for ransac_fundamental_matrix(), F matches inliers: \" + verify(test_ransac.test_ransac_fundamental_matrix_error))\n",
    "print(\"Test for ransac_fundamental_matrix(), F matches all points: \" + verify(test_ransac.test_ransac_fundamental_matrix_fit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally we will put it all together.\n",
    "\n",
    "The code below will call SIFTNet, determine the number RANSAC iterations using your function, and run your RANSAC calculating the fundamental matrix with your function at each pass. You shouldn't have to implement any new functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from feature_matching.SIFTNet import get_siftnet_features\n",
    "from feature_matching.utils import load_image, PIL_resize, rgb2gray\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Rushmore\n",
    "image1 = load_image('../data/2a_rushmore.jpg')\n",
    "image2 = load_image('../data/2b_rushmore.jpg')\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "#convert images to tensor\n",
    "tensor_type = torch.FloatTensor\n",
    "torch.set_default_tensor_type(tensor_type)\n",
    "to_tensor = transforms.ToTensor()\n",
    "image_input1 = to_tensor(image1_bw).unsqueeze(0)\n",
    "image_input2 = to_tensor(image2_bw).unsqueeze(0)\n",
    "\n",
    "from feature_matching.HarrisNet import get_interest_points\n",
    "from feature_matching.utils import show_interest_points\n",
    "x1, y1, _ = get_interest_points(image_input1.float())\n",
    "x2, y2, _ = get_interest_points(image_input2.float())\n",
    "\n",
    "x1, x2 = x1.detach().numpy(), x2.detach().numpy()\n",
    "y1, y2 = y1.detach().numpy(), y2.detach().numpy()\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))\n",
    "image1_features = get_siftnet_features(image_input1, x1, y1)\n",
    "image2_features = get_siftnet_features(image_input2, x2, y2)\n",
    "\n",
    "from feature_matching.student_feature_matching import match_features\n",
    "matches, confidences = match_features(image1_features, image2_features, x1, y1, x2, y2)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))\n",
    "\n",
    "from feature_matching.utils import show_correspondence_circles, show_correspondence_lines\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.title('Proposed Matches'); plt.imshow(c2)\n",
    "\n",
    "from proj3_code.ransac import ransac_fundamental_matrix\n",
    "# print(image1_features.shape, image2_features.shape)\n",
    "num_features = min([len(image1_features), len(image2_features)])\n",
    "x0s = np.zeros((len(matches), 2))\n",
    "x1s = np.zeros((len(matches), 2))\n",
    "x0s[:,0] = x1[matches[:, 0]]\n",
    "x0s[:,1] = y1[matches[:, 0]]\n",
    "x1s[:,0] = x2[matches[:, 1]]\n",
    "x1s[:,1] = y2[matches[:, 1]]\n",
    "# print(image1_pts.shape)\n",
    "F, matches_x0, matches_x1 = ransac_fundamental_matrix(x0s, x1s)\n",
    "print(F)\n",
    "# print(matches_x0)\n",
    "# print(matches_x1)\n",
    "\n",
    "from proj3_code.utils import draw_epipolar_lines\n",
    "# Draw the epipolar lines on the images and corresponding matches\n",
    "match_image = show_correspondence_lines(image1, image2,\n",
    "                                   matches_x0[:num_pts_to_visualize, 0], matches_x0[:num_pts_to_visualize, 1],\n",
    "                                   matches_x1[:num_pts_to_visualize, 0], matches_x1[:num_pts_to_visualize, 1])\n",
    "plt.figure(); plt.title('True Matches'); plt.imshow(match_image)\n",
    "draw_epipolar_lines(F, image1, image2, matches_x0, matches_x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the fundamental matrix by drawing the epipolar lines as shown above. As we learned these lines are the projections of the points in one image onto the other image. The point where they intersect is where the other picture was taken. \n",
    "\n",
    "### Create your own unit test\n",
    "To verify all of your own work, **create** your *own* unit test which will plot the epipolar lines on two images that *you* will take and verify that your algorithms found a valid fundamental matrix by confirming that the epipolar lines intersect where the second image was taken (this should be *clearly visible* in the image frame - see the example). \n",
    "\n",
    "To get this to work well, you will need to consider what properties are necessary in two images for both SIFT features and for the fundamental matrix to work well. For example, are our methods robust against all possible transformations, occlusion, and image features.\n",
    "\n",
    "Hint: use the code in the previous cell and input your own images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw epipolar lines and visualize on images with second camera in image\n",
    "from ransac import test_with_epipolar_lines\n",
    "\n",
    "test_with_epipolar_lines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
