{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CS 4476 Project 2: Local Feature Matching](https://dellaert.github.io/19F-4476/proj2.html)\n",
    "\n",
    "This iPython notebook:  \n",
    "(1) Loads and resizes images  \n",
    "(2) Finds interest points in those images                 (you code this)  \n",
    "(3) Describes each interest point with a local feature    (you code this)  \n",
    "(4) Finds matching features                               (you code this)  \n",
    "(5) Visualizes the matches  \n",
    "(6) Evaluates the matches based on ground truth correspondences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%matplotlib notebook\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from proj2_code.utils import load_image, PIL_resize, rgb2gray\n",
    "from IPython.core.debugger import set_trace\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('../data/1a_notredame.jpg')\n",
    "image2 = load_image('../data/1b_notredame.jpg')\n",
    "eval_file = '../ground_truth/notredame.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('../data/2a_rushmore.jpg')\n",
    "# image2 = load_image('../data/2b_rushmore.jpg')\n",
    "# eval_file = '../ground_truth/rushmore.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('../data/3a_gaudi.jpg')\n",
    "# image2 = load_image('../data/3b_gaudi.jpg')\n",
    "# eval_file = '../ground_truth/gaudi.pkl'\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "#convert images to tensor\n",
    "tensor_type = torch.FloatTensor\n",
    "torch.set_default_tensor_type(tensor_type)\n",
    "to_tensor = transforms.ToTensor()\n",
    "\n",
    "image_input1 = to_tensor(image1_bw).unsqueeze(0)\n",
    "image_input2 = to_tensor(image2_bw).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Harris Corner Detector \n",
    "## Find distinctive points in each image (Szeliski 4.1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verify each layer in the code, this will check if your implementation is correct or not.\n",
    "\n",
    "## Do not modify the constructor of any layer (i.e. to take some custom arguments\n",
    "## as input)\n",
    "\n",
    "from unit_tests.harris_unit_test import (\n",
    "    test_ImageGradientsLayer,\n",
    "    test_ChannelProductLayer, \n",
    "    test_SecondMomentMatrixLayer, \n",
    "    test_CornerResponseLayer, \n",
    "    test_NMSLayer,\n",
    "    verify\n",
    ")\n",
    "\n",
    "print('ImageGradientsLayer:', verify(test_ImageGradientsLayer))\n",
    "print('ChannelProductLayer:', verify(test_ChannelProductLayer))\n",
    "print('SecondMomentMatrixLayer:', verify(test_SecondMomentMatrixLayer))\n",
    "print('CornerResponseLayer:', verify(test_CornerResponseLayer) )\n",
    "print('NMSLayer:', verify(test_NMSLayer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.HarrisNet import get_interest_points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will call get_interest_points function in HarrisNet.py to detect 'interesting' points in the images. \n",
    "\n",
    "**IMPORTANT**\n",
    "Make sure to add your code in get_interest_points function to call Harris Corner Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import show_interest_points\n",
    "\n",
    "x1, y1, _ = get_interest_points(image_input1)\n",
    "x2, y2, _ = get_interest_points(image_input2)\n",
    "\n",
    "x1, x2 = x1.detach().numpy(), x2.detach().numpy()\n",
    "y1, y2 = y1.detach().numpy(), y2.detach().numpy()\n",
    "\n",
    "# Visualize the interest points\n",
    "c1 = show_interest_points(image1, x1, y1)\n",
    "c2 = show_interest_points(image2, x2, y2)\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.figure(); plt.imshow(c2)\n",
    "print('{:d} corners in image 1, {:d} corners in image 2'.format(len(x1), len(x2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sift Feature Descriptor\n",
    "## Create feature vectors at each interest point (Szeliski 4.1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.SIFTNet import (\n",
    "    angles_to_vectors_2d_pytorch,\n",
    "    HistogramLayer,\n",
    "    SubGridAccumulationLayer,\n",
    "    SIFTOrientationLayer,\n",
    "    get_sift_subgrid_coords,\n",
    "    get_siftnet_features\n",
    ")\n",
    "from proj2_code.torch_layer_utils import ImageGradientsLayer\n",
    "from unit_tests.sift_unit_test import (\n",
    "    test_angles_to_vectors_2d_pytorch,\n",
    "    test_HistogramLayer,\n",
    "    test_ImageGradientsLayer,\n",
    "    test_SubGridAccumulationLayer,\n",
    "    test_SIFTOrientationLayer,\n",
    "    test_get_sift_subgrid_coords,\n",
    "    test_SIFTNet,\n",
    "    test_get_siftnet_features\n",
    ")\n",
    "\n",
    "print('angles_to_vectors_2d_pytorch:', verify(test_angles_to_vectors_2d_pytorch))\n",
    "print('HistogramLayer:', verify(test_HistogramLayer))\n",
    "print('ImageGradientsLayer:', verify(test_ImageGradientsLayer))\n",
    "print('SIFTOrientationLayer:', verify(test_SIFTOrientationLayer) )\n",
    "print('SIFTNet:', verify(test_SIFTNet) )\n",
    "print('SubGridAccumulationLayer:', verify(test_SubGridAccumulationLayer))\n",
    "print('get_sift_subgrid_coords:', verify(test_get_sift_subgrid_coords) )\n",
    "print('get_siftnet_features:', verify(test_get_siftnet_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image1_features = get_siftnet_features(image_input1, x1, y1)\n",
    "image2_features = get_siftnet_features(image_input2, x2, y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match features (Szeliski 4.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your feature matching implementation\n",
    "from unit_tests.feature_match_test import test_feature_matching, test_compute_dists\n",
    "print('compute_dists:', verify(test_compute_dists))\n",
    "print('feature_matching:', verify(test_feature_matching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from student_feature_matching import match_features\n",
    "matches, confidences = match_features(image1_features, image2_features, x1, y1, x2, y2)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(x1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You might want to set 'num_pts_to_visualize' and 'num_pts_to_evaluate' to some constant (e.g. 100) once you start detecting hundreds of interest points, otherwise things might get too cluttered. You could also threshold based on confidence.  \n",
    "  \n",
    "There are two visualization functions below. You can comment out one of both of them if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.utils import show_correspondence_circles, show_correspondence_lines\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 100\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c1)\n",
    "plt.savefig('../results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    x1[matches[:num_pts_to_visualize, 0]], y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    x2[matches[:num_pts_to_visualize, 1]], y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(); plt.imshow(c2)\n",
    "plt.savefig('../results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the function below if you are not testing on the Notre Dame, Episcopal Gaudi, and Mount Rushmore image pairs--this evaluation function will only work for those which have ground truth available.  \n",
    "  \n",
    "You can use `annotate_correspondences/collect_ground_truth_corr.py` to build the ground truth for other image pairs if you want, but it's very tedious. It would be a great service to the class for future years, though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj2_code.utils import evaluate_correspondence\n",
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 100\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        x1[matches[:num_pts_to_evaluate, 0]], y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        x2[matches[:num_pts_to_evaluate, 1]], y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(); plt.imshow(c)\n",
    "plt.savefig('../results/eval.jpg', dpi=1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
