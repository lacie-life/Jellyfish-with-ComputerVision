{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "fiK4QNWKDvWJ",
    "outputId": "3472566e-5e30-47b8-a3f4-b67bf3a89af0"
   },
   "source": [
    "# [Scene Recognition with Deep Learning](https://dellaert.github.io/19F-4476/proj6.html)\n",
    "Remember in the previous project, where you have tried with a bunch of old-school techniques for image classification on the 15-scene data? Well for this project we are going to focus on the same task, but with the state-of-the-art approach: deep learning.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "1. Construct the fundamental pipeline for performing deep learning using PyTorch;\n",
    "2. Understand the concepts behind different layers, optimizers, and learning schedules;\n",
    "3. Experiment with different models and observe the performance.\n",
    "\n",
    "The starter code is mostly initialized to 'placeholder' just so that the starter\n",
    "code does not crash when run unmodified and you can get a preview of how\n",
    "results are presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "g1dqr6qSBpE2",
    "outputId": "2a6bb055-8fb2-4b0e-b137-31efc09c375b"
   },
   "outputs": [],
   "source": [
    "from proj6_code.runner import Trainer\n",
    "from proj6_code.optimizer import get_optimizer\n",
    "from proj6_code.simple_net import SimpleNet\n",
    "from proj6_code.simple_net_dropout import SimpleNetDropout\n",
    "from proj6_code.my_alexnet import MyAlexNet\n",
    "from proj6_code.image_loader import ImageLoader\n",
    "from proj6_code.data_transforms import get_fundamental_transforms, get_data_augmentation_transforms\n",
    "from proj6_code.stats_helper import compute_mean_and_std\n",
    "\n",
    "import torch\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from proj6_unit_tests.test_base import verify\n",
    "from proj6_unit_tests.test_stats_helper import test_mean_and_variance\n",
    "from proj6_unit_tests.test_image_loader import test_dataset_length, test_unique_vals, test_class_values, test_load_img_from_path\n",
    "from proj6_unit_tests.test_data_transforms import test_fundamental_transforms\n",
    "from proj6_unit_tests.test_dl_utils import test_predict_labels, test_compute_loss\n",
    "from proj6_unit_tests.test_simple_net import test_simple_net\n",
    "from proj6_unit_tests.test_simple_net_dropout import test_simple_net_dropout\n",
    "from proj6_unit_tests.test_my_alexnet import test_my_alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GjE0jIc5BpFN"
   },
   "outputs": [],
   "source": [
    "is_cuda = True\n",
    "is_cuda = is_cuda and torch.cuda.is_available() # will turn off cuda if the machine doesnt have a GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGSv2QfBBpFZ"
   },
   "source": [
    "## Part 1: Train a SimpleNet\n",
    "To train a network in PyTorch, we need 4 components:\n",
    "1. **Dataset** - an object which can load the data and labels given an index.\n",
    "2. **Model** - an object that contains the network architecture definition.\n",
    "3. **Loss function** - a function that measures how far the network output is from the ground truth label.\n",
    "4. **Optimizer** - an object that optimizes the network parameters to reduce the loss value.\n",
    "\n",
    "### Part 1.1: Datasets\n",
    "One crucial aspect of deep learning is to perform data preprocessing. In this project we are going to \"zero-center\" and \"normalize\" the dataset. \n",
    "\n",
    "### Compute mean and standard deviation of the dataset\n",
    "To begin with, fill in the `compute_mean_and_std()` in `stats_helper.py` to compute the **mean** and **standard deviation** of both training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vWA_2UbjBpFd"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your mean and std computation: \", verify(test_mean_and_variance))\n",
    "dataset_mean, dataset_std = compute_mean_and_std('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xixFr8CDBpFn",
    "outputId": "267d219f-8089-4b25-9e4d-c9cc5a8699e2"
   },
   "outputs": [],
   "source": [
    "print('Dataset mean = {}, standard deviation = {}'.format(dataset_mean, dataset_std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-2TeGbrQBpFu"
   },
   "source": [
    "Now let's create the **Datasets** object to be used later. Remember back in Project 1, we have initialized such a class to load 5 images? Here the task is similar: download the data into the project folder, and then complete the `image_loader.py`. The essence is to retrieve the paths to all the images required, and be able to provide the **path** and the **class id** when given an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THRvAvluXFcS"
   },
   "outputs": [],
   "source": [
    "inp_size = (64,64)\n",
    "print(\"Testing your image loader (length):\", verify(test_dataset_length))\n",
    "print(\"Testing your image loader (values):\", verify(test_unique_vals))\n",
    "print(\"Testing your image loader (classes):\", verify(test_class_values))\n",
    "print(\"Testing your image loader (paths):\", verify(test_load_img_from_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transforms\n",
    "For this part, complete the function `get_fundamental_transforms()` in `data_transforms.py` to compile a list of fundamental transforms which:\n",
    "1. Resize the input image to the desired shape;\n",
    "2. Convert it to a tensor;\n",
    "3. Normalize them based on the computed mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your fundamental data transforms: \", verify(test_fundamental_transforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2: Model\n",
    "The data is ready! Now we are preparing to move to the actual core of deep learning: the architecture. To get you started in this part, simply define a **2-layer** model in the `simple_net.py`. Here by \"2 layers\" we mean **2 convolutional layers**, so you need to figure out the supporting utilities like ReLU, Max Pooling, and Fully Connected layers, and configure them with proper parameters to make the tensor flow.\n",
    "\n",
    "You may refer to the project [instruction page](https://dellaert.github.io/19F-4476/proj6.html) for a sample network architecture (it's the architecture TAs used in their implementation and is sufficient to get you pass Part 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jvVL-ap0BpFx"
   },
   "outputs": [],
   "source": [
    "print(\"Testing your SimpleNet architecture: \", verify(test_simple_net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = SimpleNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "When defining your model architecture, also initialize the `loss_criterion` variable there. Remeber this is multi-class classification problem, and think about what loss function might be useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(simple_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "Next, **initialize the following cell with proper values for learning rate and weight decay** (you can come back and tune these values for better performance once the trainer section is done), and then fill in the `optimizer.py` to initialize a basic optimization function; this should only take a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V2cwtK5PBpF7"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"sgd\",\n",
    "  \"lr\": 1e-10,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P0CrYZa4BpGE"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model, optimizer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.3: Trainer\n",
    "Next we define the trainer for the model; to start, complete the `predict_labels()` and `compute_loss()` in `dl_utils.py`: given a model, compute the corresponding predictions and loss respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your trainer (model prediction): \", verify(test_predict_labels))\n",
    "print(\"Testing your trainer (loss values): \", verify(test_compute_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass in the model, optimizer, transforms for both the training and testing datasets into the trainer, and proceed to the next cell to train it. If you have implemented everything correctly, you should be seeing a decreasing loss value.\n",
    "\n",
    "**Note** that your CPU should be sufficient to handle the training process for all networks in this project, and the following training cells will take less than 5 minutes; you may also want to decrease the value for `num_epochs` and quickly experiment with your parameters. The default value of **30** is good enough to get you around the threshold for Part 1, and you are free to increase it a bit and adjust other parameters in this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiGOvPJfBpGO"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_dir='../data/', \n",
    "                  model = simple_model,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = 'model_checkpoints/simple_net',\n",
    "                  train_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "paNLyU5cBpGX",
    "outputId": "56af8728-d91c-4886-c73b-d12044ffe40a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have finished the training process, now plot out the loss and accuracy history. You can also check out the final accuracy for both training and testing data. Copy the accuracy plots and values onto the report, and answer the questions there. Note that you are required to obtain a **50%** testing accuracy to receive full credits for Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "z0b_WwJhBpGf",
    "outputId": "8d299888-33c6-4234-dc78-5585c9888057",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8epn0IBmBpGn",
    "outputId": "a9e4d15d-9ac9-4b38-cf26-86a93f734495",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {}; Validation Accuracy = {}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G7hE1vidBpGt"
   },
   "source": [
    "## Part 2: Overfitting\n",
    "Feeling good? We have easily obtained a 50% accuracy on the testing data with a very simple model; this is already the highest standard for the previous project. Feeling even better for the training accuracy right? More than 90% (if you have implemented everything correctly). But should you?\n",
    "\n",
    "Our final accuracies for training and testing data differ a lot from each other, which indicates that the model we have defined **fits too well with the training data, but is unable to generalize well on data it has never seen before**: this is often regarded as **overfitting**. In this section we are going to apply 2 techniques to tackle with it: adjusting both data and model.\n",
    "\n",
    "### Part 2.1: Jitter, Random Flip, and Normalization\n",
    "One common technique to increase the \"variability\" of the data is to **augment** it. Firstly, we don't have a huge amount of data, so let's \"jitter\" based on it; secondly, when you mirror an image of a **kitchen**, you can tell that the mirrored image is still a kitchen. Hence, finsih the `get_data_augmentation_transforms()` function in `data_transforms.py`: you may first copy your existing fundamental transform implementation into this function, and then insert a couple of other transforms which help you do the above adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ech4Y22OXOui"
   },
   "outputs": [],
   "source": [
    "inp_size = (64,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2: Dropout\n",
    "\"Dropout\" is a technique commonly used to regularize the network. It randomly turns off the connection between neurons inside the network and prevent the network from relying too much on a specific neuron. Follow the instruction and finish the `simple_net_dropout.py` with your previous SimpleNet model, plus the dropout layer, and lastly re-run the training process as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your SimpleNetDropout architecture: \", verify(test_simple_net_dropout))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nlgm5eM-BpGz"
   },
   "outputs": [],
   "source": [
    "simple_model_dropout = SimpleNetDropout()\n",
    "print(simple_model_dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous part, **initialize the following cell with proper values for learning rate and weight decay**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btKIvIrdBpG5"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"sgd\",\n",
    "  \"lr\": 1e-10,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "peqS_C6QBpG_"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(simple_model_dropout, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ExoLylurBpHH"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_dir='../data/', \n",
    "                  model = simple_model_dropout,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = 'model_checkpoints/simple_net_dropout',\n",
    "                  train_data_transforms = get_data_augmentation_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will take longer than Part 1, as now we have more data (and more variability), and the model is slightly more complicated than before as well; however, it should finish within 10~15 minutes anyway, and the default `num_epochs` is also good enough as a starting point for you to pass this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 548
    },
    "colab_type": "code",
    "id": "-ljUl4UnBpHN",
    "outputId": "c3542fae-c2dc-495b-dd49-d5bb85c7fd79",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous part, now plot out the loss and accuracy history. You'll need to pass a threshold of **55%** to receive full credits for this part. Also copy the plots onto the report, and answer the questions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Gdh9AvHIBpHW",
    "outputId": "4fd9e6eb-ebcf-4bbb-a9e6-79db49777661",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "6SLuc3zmBpHd",
    "outputId": "4069ff19-8bcc-44b3-9cd5-8bc8a63e626d"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {}; Validation Accuracy = {}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "amh1wxlJBpHj"
   },
   "source": [
    "## Part 3: AlexNet\n",
    "You can see that after the above adjustment, our model performance increases in terms of testing accuracy. Although the training accuracy drops, now it's closer to the testing values and that's more natural in terms of performance. But we are not satisfied with the final performance yet. Our model, in the end, is still a 2-layer SimpleNet and it might be capable of capturing some features, but could be improved a lot if we go **deeper**. In this part we are going to see the power of a famous model: AlexNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttbhBZ7CXUng"
   },
   "outputs": [],
   "source": [
    "inp_size = (224, 224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.1 & 3.2: Fine-tuning the AlexNet\n",
    "Now switch to `my_alexnet.py`, and define a AlexNet which can be fit onto our dataset: PyTorch has provided us with pre-trained models like AlexNet, so what you want to do is to load the model first, and then adjust some of the layers such that it fits with our own dataset, instead of outputing scores to 1000 classes from the original AlexNet model.\n",
    "\n",
    "After you have defined the correct architecture of the model, make some tweaks to the existing layers: **freeze** the **convolutional** layers and first 2 **linear** layers so we don't update the weights of them; more details can be found in the instruction webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing your AlexNet architecture: \", verify(test_my_alexnet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CBoLgRrlBpHl"
   },
   "outputs": [],
   "source": [
    "my_alexnet = MyAlexNet()\n",
    "print(my_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J6AYkHAgBpHw"
   },
   "outputs": [],
   "source": [
    "# TODO: add a decent initial setting and tune from there\n",
    "optimizer_config = {\n",
    "  \"optimizer_type\": \"sgd\",\n",
    "  \"lr\": 1e-10,\n",
    "  \"weight_decay\": 1e-1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72N8uwsZBpIA"
   },
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(my_alexnet, optimizer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DtCIaTMmBpIK"
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(data_dir='../data/', \n",
    "                  model = my_alexnet,\n",
    "                  optimizer = optimizer,\n",
    "                  model_dir = 'model_checkpoints/myalexnet/',\n",
    "                  train_data_transforms = get_data_augmentation_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  test_data_transforms = get_fundamental_transforms(inp_size, dataset_mean, dataset_std),\n",
    "                  batch_size = 32,\n",
    "                  load_from_disk = False,\n",
    "                  cuda = is_cuda\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following training cell will take roughly 20 minutes or slightly more using CPU (but possibly under 5 minute using GPU depending on the batch size; the TAs got it within 3 minutes on a GTX1060)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "CAcncwLPBpIQ",
    "outputId": "7a341877-2628-4acf-e99a-a1c96ff51fd8"
   },
   "outputs": [],
   "source": [
    "trainer.train(num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like both previous sections, you are required to pass a threshold of **85%** for this part. Copy the plots and values onto the report and answer questions accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Cimj95G_BpIU",
    "outputId": "f2356e0d-b2c4-4a13-ff63-4516ee5193d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.plot_loss_history()\n",
    "trainer.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "qMI3CdEuBpIb",
    "outputId": "ffccde71-d62e-4eae-cd2e-241847b9ee23"
   },
   "outputs": [],
   "source": [
    "train_accuracy = trainer.train_accuracy_history[-1]\n",
    "validation_accuracy = trainer.validation_accuracy_history[-1]\n",
    "print('Train Accuracy = {}; Validation Accuracy = {}'.format(train_accuracy, validation_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these we have concluded the last project of CS4476 Computer Vision. Things might be hard along the way, but we hope you enjoyed this journey and have learnt something in this field. Our team has learnt a lot from you guys as well, so thank you and wish you all the best in your future endeavors!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dl.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
