{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, a majority of objects have a similar shape – for example, in a majority of\n",
    "cases, a bounding box corresponding to an image of a person will have a greater\n",
    "height than width, and a bounding box corresponding to the image of a truck will\n",
    "have a greater width than height. Thus, we will have a decent idea of the height and\n",
    "width of the objects present in an image even before training the model (by inspecting\n",
    "the ground truths of bounding boxes corresponding to objects of various classes).\n",
    "Furthermore, in some images, the objects of interest might be scaled – resulting in a\n",
    "much smaller or much greater height and width than average – while still\n",
    "maintaining the aspect ratio (that is, height/width).\n",
    "Once we have a decent idea of the aspect ratio and the height and width of objects\n",
    "(which can be obtained from ground truth values in the dataset) present in our\n",
    "images, we define the anchor boxes with heights and widths representing the\n",
    "majority of objects' bounding boxes within our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, this is obtained by employing K-means clustering on top of the ground\n",
    "truth bounding boxes of objects present in images.\n",
    "Now that we understand how anchor boxes' heights and widths are obtained, we will\n",
    "learn about how to leverage them in the process:\n",
    "\n",
    "1. Slide each anchor box over an image from top left to bottom right.\n",
    "\n",
    "2. The anchor box that has a high intersection over union (IoU) with the\n",
    "object will have a label that mentions that it contains an object, and the\n",
    "others will be labeled 0:\n",
    "    - We can modify the threshold of the IoU by mentioning that if the IoU\n",
    "    is greater than a certain threshold, the object class is 1; if it is less than\n",
    "    another threshold, the object class is 0, and it is unknown otherwise.\n",
    "\n",
    "Once we obtain the ground truths as defined here, we can build a model that can\n",
    "predict the location of an object and also the offset corresponding to the anchor box to\n",
    "match it with ground truth.\n",
    "\n",
    "![imgs](./imgs/o5.png)\n",
    "\n",
    "In the preceding image, we have two anchor boxes, one that has a greater height than\n",
    "width and the other with a greater width than height, to correspond to the objects\n",
    "(classes) in the image – a person and a car.\n",
    "\n",
    "We slide the two anchor boxes over the image and note the locations where the IoU of\n",
    "the anchor box with the ground truth is the highest and denote that this particular\n",
    "location contains an object while the rest of the locations do not contain an object.\n",
    "\n",
    "In addition to the preceding two anchor boxes, we would also create anchor boxes\n",
    "with varying scales so that we accommodate the differing scales at which an object\n",
    "can be presented within an image. An example of how the different scales of anchor\n",
    "boxes look follows:\n",
    "\n",
    "![imgs](./imgs/o6.png)\n",
    "\n",
    "Note that all the anchor boxes have the same center but different aspect ratios or\n",
    "scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Proposal Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a scenario where we have a 224 x 224 x 3 image. Furthermore, let's say that\n",
    "the anchor box is of shape 8 x 8 for this example. If we have a stride of 8 pixels, we are\n",
    "fetching 224/8 = 28 crops of a picture for every row – essentially 28*28 = 576 crops\n",
    "from a picture. We then take each of these crops and pass through a Region Proposal\n",
    "Network model (RPN) that indicates whether the crop contains an image. Essentially,\n",
    "an RPN suggests the likelihood of a crop containing an object.\n",
    "\n",
    "Let's compare the output of selectivesearch and the output of an RPN.\n",
    "\n",
    "selectivesearch gives a region candidates based on a set of computations on top\n",
    "of pixel values. However, an RPN generates region candidates based on the anchor\n",
    "boxes and the strides with which anchor boxes are slid over the image. Once we\n",
    "obtain the region candidates using either of these two methods, we identify the\n",
    "candidates that are most likely to contain an object.\n",
    "\n",
    "While region proposal generation based on selectivesearch is done outside of the\n",
    "neural network, we can build an RPN that is a part of the object detection network.\n",
    "Using an RPN, we are now in a position where we don't have to perform unnecessary\n",
    "computations to calculate region proposals outside of the network. This way, we have\n",
    "a single model to identify regions, identify classes of objects in image, and identify\n",
    "their corresponding bounding box locations.\n",
    "\n",
    "Next, we will learn how an RPN identifies whether a region candidate (a crop\n",
    "obtained after sliding an anchor box) contains an object or not. In our training data,\n",
    "we would have the ground truth correspond to objects. We now take each region\n",
    "candidate and compare with the ground truth bounding boxes of objects in an image\n",
    "to identify whether the IoU between a region candidate and a ground truth bounding\n",
    "box is greater than a certain threshold. If the IoU is greater than a certain threshold\n",
    "(say, 0.5), the region candidate contains an object, and if the IoU is less than a\n",
    "threshold (say 0.1), the region candidate does not contain an object and all the\n",
    "candidates that have an IoU between the two thresholds (0.1 - 0.5) are ignored while\n",
    "training.\n",
    "\n",
    "Once we train a model to predict if the region candidate contains an object, we then\n",
    "perform non-max suppression, as multiple overlapping regions can contain an object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RPN trains a model to enable it to identify region proposals with a\n",
    "high likelihood of containing an object by performing the following steps:\n",
    "1. Slide anchor boxes of different aspect ratios and sizes across the image to\n",
    "fetch crops of an image.\n",
    "2. Calculate the IoU between the ground truth bounding boxes of objects in\n",
    "the image and the crops obtained in the previous step.\n",
    "3. Prepare the training dataset in such a way that crops with an IoU greater\n",
    "than a threshold contain an object and crops with an IoU less than a\n",
    "threshold do not contain an object.\n",
    "4. Train the model to identify regions that contain an object.\n",
    "5. Perform non-max suppression to identify the region candidate that has the\n",
    "highest probability of containing an object and eliminate other region\n",
    "candidates that have a high overlap with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification and regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have learned about the following steps in order to identify objects and\n",
    "perform offsets to bounding boxes:\n",
    "\n",
    "1. Identify the regions that contain objects.\n",
    "\n",
    "2. Ensure that all the feature maps of regions, irrespective of the regions'\n",
    "shape, are exactly the same using region of interest (RoI) pooling (which\n",
    "we learned about in the previous chapter).\n",
    "\n",
    "Two issues with these steps are as follows:\n",
    "\n",
    "1. The region proposals do not correspond tightly over the object (IoU>0.5 is\n",
    "the threshold we had in the RPN).\n",
    "\n",
    "2. We identified whether the region contains an object or not, but not the class\n",
    "of the object located in the region.\n",
    "\n",
    "We address these two issues in this section, where we take the uniformly shaped\n",
    "feature map obtained previously and pass it through a network. We expect the\n",
    "network to predict the class of the object contained within the region and also the\n",
    "offsets corresponding to the region to ensure that the bounding box is as tight as\n",
    "possible around the object in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![imgs](./imgs/o7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding diagram, we are taking the output of RoI pooling as input (the 7 x 7 x\n",
    "512 shape), flattening it, and connecting to a dense layer before predicting two\n",
    "different aspects:\n",
    "\n",
    "1. Class of object in the region\n",
    "\n",
    "2. Amount of offset to be done on the predicted bounding boxes of the region\n",
    "to maximize the IoU with the ground truth\n",
    "\n",
    "Hence, if there are 20 classes in the data, the output of the neural network contains a\n",
    "total of 25 outputs – 21 classes (including the background class) and the 4 offsets to be\n",
    "applied to the height, width, and two center coordinates of the bounding box.\n",
    "\n",
    "Now that we have learned the different components of an object detection pipeline,\n",
    "let's summarize it with the following diagram:\n",
    "\n",
    "![imgs](./imgs/o8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
