{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ResNet architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While building too deep a network, there are two problems. In forward propagation,\n",
        "the last few layers of the network have almost no information about what the original\n",
        "image was. In backpropagation, the first few layers near the input hardly get any\n",
        "gradient updates due to vanishing gradients (in other words, they are almost zero).\n",
        "To solve both problems, residual networks (ResNet) use a highway-like connection\n",
        "that transfers raw information from the previous few layers to the later layers. In\n",
        "theory, even the last layer will have the entire information of the original image due\n",
        "to this highway network. And because of the skipping layers, the backward gradients\n",
        "will flow freely to the initial layers with little modification.\n",
        "\n",
        "The term residual in the residual network is the additional information that the\n",
        "model is expected to learn from the previous layer that needs to be passed on to the\n",
        "next layer.\n",
        "\n",
        "A typical residual block appears as follows:\n",
        "\n",
        "![imgs](./imgs/trans1.png)\n",
        "\n",
        "As you can see, while so far, we have been interested in extracting the F(x) value,\n",
        "where x is the value coming from the previous layer, in the case of a residual\n",
        "network, we are extracting not only the value after passing through the weight layers,\n",
        "which is F(x), but are also summing up F(x) with the original value, which is x.\n",
        "\n",
        "So far, we have been using standard layers that performed either linear or\n",
        "convolution transformations F(x) along with some non-linear activation. Both of\n",
        "these operations in some sense destroy the input information. For the first time, we\n",
        "are seeing a layer that not only transforms the input, but also preserves it, by adding\n",
        "the input directly to the transformation â€“ F(x) + x . This way, in certain scenarios,\n",
        "the layer has very little burden in remembering what the input is, and can focus on\n",
        "learning the correct transformation for the task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cuS9OIFRPtEW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S339gpWBP06u"
      },
      "outputs": [],
      "source": [
        "class ResLayer(nn.Module):\n",
        "     def __init__(self,ni,no,kernel_size,stride=1):\n",
        "        super(ResLayer, self).__init__()\n",
        "        padding = kernel_size - 2\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ni, no, kernel_size, stride, \n",
        "                      padding=padding),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x) + x\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyOTLHszFiFWrGkTp/f0kcnn",
      "include_colab_link": true,
      "name": "Resnet_block_architecture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
