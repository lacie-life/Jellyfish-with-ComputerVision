{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The basics of reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning (RL) is an area of machine learning concerned with how\n",
    "software <b> agents </b> ought to take <b> actions </b> in a given <b> state </b> of an <b> environment </b> to maximize\n",
    "the notion of cumulative <b> reward </b>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how RL helps, let's consider a simple scenario. Imagine that you are\n",
    "playing chess against a computer (in our case, the computer is an agent that has\n",
    "learned/is learning how to play chess). The setup (rules) of the game constitutes the\n",
    "environment. Furthermore, as we make a move (take an action), the state of the\n",
    "board (the location of various pieces on the chessboard) changes. At the end of the\n",
    "game, depending on the result, the agent gets a reward. The objective of the agent is\n",
    "to maximize the reward.\n",
    "\n",
    "If the machine (agent1) is playing against a human, the number of games that it can\n",
    "play is finite (depending on the number of games the human can play). This might\n",
    "create a bottleneck for the agent to learn well. However, what if agent1 (the agent that\n",
    "is learning the game) can play against agent2 (agent2 could be another agent that is\n",
    "learning chess or it could be a piece of chess software that has been pre-programmed\n",
    "to play the game well)? Theoretically, the agents can play infinite games with each\n",
    "other, which results in maximizing the opportunity to learn to play the game well.\n",
    "This way, by playing multiple games with each other, the learning agent is likely to\n",
    "learn how to address the different scenarios/states of the game well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand the process that the learning agent will follow to learn well:\n",
    "\n",
    "1. Initially, the agent takes a random action in a given state.\n",
    "\n",
    "2. The agent stores the action it has taken in various states within a game in\n",
    "memory.\n",
    "\n",
    "3. Then, the agent associates the result of the action in various states with a\n",
    "reward.\n",
    "\n",
    "4. After playing multiple games, the agent can correlate the action in a state to\n",
    "a potential reward by replaying its experiences.\n",
    "\n",
    "Next comes the question of quantifying the value that corresponds to taking an action\n",
    "in a given state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the state value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how to quantify the value of a state, let's use a simple scenario where\n",
    "we will define the environment and objective as follows:\n",
    "\n",
    "![rl](../imgs/rl0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is a grid with two rows and three columns. The agent starts at the\n",
    "Start cell and it achieves its objective (rewarded with a score of +1) if the agent reaches\n",
    "the bottom-right grid cell. The agent does not get a reward if it goes to any other cell.\n",
    "The agent can take an action by going to the right, left, bottom, or up, depending on\n",
    "the feasibility of the action (the agent can go to the right or to the bottom in the start\n",
    "grid cell, for example). The reward of reaching any of the remaining cells other than\n",
    "the bottom-right cell is 0.\n",
    "\n",
    "By using this information, let's calculate the <b> value </b> of a cell (the state that the agent is\n",
    "in, in a given snapshot). Given that some energy is spent moving from one cell to\n",
    "another, we discount the value of reaching a cell by a factor of γ, where γ takes care of\n",
    "the energy that's spent in moving from one cell to another. Furthermore, the\n",
    "introduction of γ results in the agent learning to play well sooner. With this, let's\n",
    "formalize the Bellman equation, which helps in calculating the value of a cell:\n",
    "\n",
    "![rl](../imgs/rl1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the preceding equation in place, let's calculate the values of all cells <b> (once the\n",
    "optimal actions in a state have been identified) </b> with the value of γ being 0.9 (the\n",
    "typical value of γ is between 0.9 and 0.99):\n",
    "\n",
    "![rl](../imgs/rl2.png)\n",
    "\n",
    "From the preceding calculations, we can understand how to calculate the values in a\n",
    "given state (cell), when given the optimal actions in that state. These are as follows for\n",
    "our simplistic scenario of reaching the terminal state:\n",
    "\n",
    "![rl](../imgs/rl3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the values in place, we expect the agent to follow a path of increasing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the state-action value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we provided a scenario where we already know that the\n",
    "agent is taking optimal actions (which is not realistic). In this section, we will look at a\n",
    "scenario where we can identify the value that corresponds to a state-action\n",
    "combination.\n",
    "\n",
    "In the following image, each sub-cell within a cell represents the value of taking an\n",
    "action in the cell. Initially, the cell values for various actions are as follows:\n",
    "\n",
    "![rl](../imgs/rl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in the preceding image, cell b1 (2 nd row and 2 nd column) will have a value of\n",
    "1 if the agent moves right from the cell (as it corresponds to the terminal cell); the\n",
    "other actions result in a value of 0. X represents that the action is not possible and\n",
    "hence no value is associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over four iterations (steps), the updated cell values for the actions in the given state\n",
    "are as follows:\n",
    "\n",
    "![rl](../imgs/rl5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would then go through multiple iterations to provide the optimal action that\n",
    "maximizes value at each cell.\n",
    "\n",
    "Let's understand how to obtain the cell values in the second table (Iteration 2 in the\n",
    "preceding image). Let's narrow this down to 0.3, which was obtained by taking the\n",
    "downward action when present in the 1st row and 2nd column of the second table.\n",
    "When the agent takes the downward action, there is a 1/3 chance of it taking the\n",
    "optimal action in the next state. Hence, the value of taking a downward action is as\n",
    "follows:\n",
    "\n",
    "![rl](../imgs/rl6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar manner, we can obtain the values of taking different possible actions in differnt cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Q in Q-learning or Q-value represents the quality of an action. Let's learn how to\n",
    "calculate it:\n",
    "\n",
    "![rl](../imgs/rl7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know that we must keep updating the state-action value of a given state\n",
    "until it is saturated. Hence, we'll modify the preceding formula like so:\n",
    "\n",
    "![rl](../imgs/rl8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding equation, we replace 1 with the learning rate so that we can update\n",
    "the value of the action that's taken in a state more gradually:\n",
    "\n",
    "![rl](../imgs/rl9.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
