{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building a Q-table\n",
        "\n",
        "We can populate the values of the Q-table using the following strategy:\n",
        "1. Initialize the game environment and the Q-table with zeros.\n",
        "2. Take a random action and fetch the next state, reward, the flag stating\n",
        "whether the game was completed, and additional information.\n",
        "3. Update the Q-value using the Bellman equation we defined earlier.\n",
        "4. Repeat steps 2 and 3 so that there's a maximum of 50 steps in an episode.\n",
        "5. Repeat steps 2, 3, and 4 over multiple episodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Uf-9PVPXip-f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "env = gym.make('FrozenLake-v1', is_slippery=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xhL3Uf8AirCx"
      },
      "outputs": [],
      "source": [
        "action_size=env.action_space.n\n",
        "state_size=env.observation_space.n\n",
        "qtable=np.zeros((state_size,action_size))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeVgzBOkislL",
        "outputId": "d988f197-ee62-4863-ec4a-5afea6a3a499"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lacie/miniconda3/envs/conda-pytorch/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "too many values to unpack (expected 4)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m50\u001b[39m):\n\u001b[1;32m      6\u001b[0m     action\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample()\n\u001b[0;32m----> 7\u001b[0m     new_state,reward,done,info\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m      8\u001b[0m     qtable[state,action]\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m\u001b[39m*\u001b[39m(reward\u001b[39m+\u001b[39m\u001b[39m0.9\u001b[39m\u001b[39m*\u001b[39mnp\u001b[39m.\u001b[39mmax(qtable[new_state,:])\u001b[39m-\u001b[39mqtable[state,action])\n\u001b[1;32m      9\u001b[0m     state\u001b[39m=\u001b[39mnew_state\n",
            "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
          ]
        }
      ],
      "source": [
        "episode_rewards = []\n",
        "for i in range(10000):\n",
        "    state=env.reset()\n",
        "    total_rewards = 0\n",
        "    for step in range(50):\n",
        "        action=env.action_space.sample()\n",
        "        new_state,reward,done,info=env.step(action)\n",
        "        qtable[state,action]+=0.1*(reward+0.9*np.max(qtable[new_state,:])-qtable[state,action])\n",
        "        state=new_state\n",
        "        total_rewards+=reward\n",
        "    episode_rewards.append(total_rewards)\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9SOLfGnjgZb",
        "outputId": "862030db-9c6e-44fa-f844-cd98a33c7bf2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.531441   0.59049    0.59049    0.531441  ]\n",
            " [0.531441   0.         0.6561     0.59049   ]\n",
            " [0.59049    0.729      0.59049    0.6561    ]\n",
            " [0.6561     0.         0.59049    0.59049   ]\n",
            " [0.59049    0.6561     0.         0.531441  ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.81       0.         0.6561    ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.6561     0.         0.729      0.59049   ]\n",
            " [0.6561     0.81       0.81       0.        ]\n",
            " [0.729      0.9        0.         0.729     ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.80999999 0.9        0.729     ]\n",
            " [0.81       0.9        1.         0.81      ]\n",
            " [0.         0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "episode_rewards = []\n",
        "epsilon=1\n",
        "max_epsilon=1\n",
        "min_epsilon=0.01\n",
        "decay_rate=0.005\n",
        "for episode in range(1000):\n",
        "    state=env.reset()\n",
        "    total_rewards = 0\n",
        "    for step in range(50):\n",
        "        exp_exp_tradeoff=random.uniform(0,1)\n",
        "        ## Exploitation:\n",
        "        if exp_exp_tradeoff>epsilon:\n",
        "            action=np.argmax(qtable[state,:])\n",
        "        else:\n",
        "            ## Exploration\n",
        "            action=env.action_space.sample()\n",
        "        new_state,reward,done,info=env.step(action)\n",
        "        qtable[state,action]+=0.9*(reward+0.9*np.max(qtable[new_state,:])-qtable[state,action])\n",
        "        state=new_state\n",
        "        total_rewards+=reward\n",
        "    episode_rewards.append(total_rewards)\n",
        "    epsilon=min_epsilon+(max_epsilon-min_epsilon)*np.exp(decay_rate*episode)\n",
        "print(qtable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QtCWRXclCue",
        "outputId": "a9ff1f05-52a6-47a0-cb42-9ab007a73937"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------\n",
            "Episode 0\n",
            "\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2\n",
            "  (Right)\n",
            "S\u001b[41mF\u001b[0mFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "2\n",
            "  (Right)\n",
            "SF\u001b[41mF\u001b[0mF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "1\n",
            "  (Down)\n",
            "SFFF\n",
            "FH\u001b[41mF\u001b[0mH\n",
            "FFFH\n",
            "HFFG\n",
            "1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FF\u001b[41mF\u001b[0mH\n",
            "HFFG\n",
            "1\n",
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HF\u001b[41mF\u001b[0mG\n",
            "2\n",
            "Number of Steps 6\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "for episode in range(1):\n",
        "    state=env.reset()\n",
        "    step=0\n",
        "    done=False\n",
        "    print(\"-----------------------\")\n",
        "    print(\"Episode\",episode)\n",
        "    for step in range(50):\n",
        "        env.render()\n",
        "        action=np.argmax(qtable[state,:])\n",
        "        print(action)\n",
        "        new_state,reward,done,info=env.step(action) \n",
        "        if done:\n",
        "            print(\"Number of Steps\",step+1)\n",
        "            break\n",
        "        state=new_state\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djaZt5qAlQMv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Building_Q_table.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
