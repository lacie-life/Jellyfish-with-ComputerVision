{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Implementing deep Q-learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far, we have learned how to build a Q-table, which provides values that\n",
        "correspond to a given state-action combination by replaying a game – in this case, the\n",
        "Frozen Lake game – over multiple episodes. However, when the state spaces are\n",
        "continuous (such as a snapshot of a game of Pong), the number of possible state\n",
        "spaces becomes huge. We will address this in this section, as well as the ones to\n",
        "follow, using deep Q-learning. In this section, we will learn how to estimate the Q-\n",
        "value of a state-action combination without a Q-table by using a neural network\n",
        "– hence the term deep Q-learning.\n",
        "\n",
        "Compared to a Q-table, deep Q-learning leverages a neural network to map any given\n",
        "state-action (where the state can be continuous or discrete) combination to Q-values.\n",
        "\n",
        "For this exercise, we will work on the CartPole environment in Gym. Here, our task is\n",
        "to balance the CartPole for as long as possible. The following image shows what the\n",
        "CartPole environment looks like:\n",
        "\n",
        "![rl](../imgs/rl10.png)\n",
        "\n",
        "Note that the pole shifts to the left when the cart moves to the right and vice versa.\n",
        "Each state within this environment is defined using four observations, whose names\n",
        "and minimum and maximum values are as follows:\n",
        "\n",
        "![rl](../imgs/rl11.png)\n",
        "\n",
        "Note that all the observations that represent a state have continuous values.\n",
        "\n",
        "At a high level, deep Q-learning for the game of CartPole balancing works as follows:\n",
        "\n",
        "1. Fetch the input values (image of the game/metadata of the game).\n",
        "\n",
        "2. Pass the input values through a network that has as many outputs as there\n",
        "are possible actions.\n",
        "\n",
        "3. The output layers predict the action values that correspond to taking an\n",
        "action in a given state.\n",
        "\n",
        "A high-level overview of the network architecture is as follows:\n",
        "\n",
        "![rl](../imgs/rl12.png)\n",
        "\n",
        "In the preceding image, the network architecture uses the state (four observations) as\n",
        "input and the Q-value of taking left and right actions in the current state as output.\n",
        "We train the neural network as follows:\n",
        "\n",
        "1. During the exploration phase, we perform a random action that has the\n",
        "highest value in the output layer.\n",
        "\n",
        "2. Then, we store the action, the next state, the reward, and the flag stating\n",
        "whether the game was complete in memory.\n",
        "\n",
        "3. In a given state, if the game is not complete, the Q-value of taking an action\n",
        "in a given state will be calculated; that is, reward + discount factor x\n",
        "maximum possible Q-value of all actions in the next state.\n",
        "\n",
        "4. The Q-values of the current state-action combinations remain unchanged\n",
        "except for the action that is taken in step 2.\n",
        "\n",
        "5. Perform steps 1 to 4 multiple times and store the experiences.\n",
        "\n",
        "6. Fit a model that takes the state as input and the action values as the\n",
        "expected outputs (from memory and replay experience) and minimize the\n",
        "MSE loss.\n",
        "\n",
        "7. Repeat the preceding steps over multiple episodes while decreasing the\n",
        "exploration rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EAsxgBMbnOlT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lacie/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import namedtuple, deque\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3ANvz1Z_nWDz"
      },
      "outputs": [],
      "source": [
        "env = gym.make('CartPole-v1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NXyrPGJSnXpp"
      },
      "outputs": [],
      "source": [
        "class DQNetwork(nn.Module):\n",
        "    def __init__(self, state_size, action_size):\n",
        "        super(DQNetwork, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(state_size, 24)\n",
        "        self.fc2 = nn.Linear(24, 24)\n",
        "        self.fc3 = nn.Linear(24, action_size)\n",
        "        \n",
        "    def forward(self, state):       \n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Fxxt2zRGnczv"
      },
      "outputs": [],
      "source": [
        "class Agent():\n",
        "    def __init__(self, state_size, action_size):\n",
        "        \n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(0)\n",
        "\n",
        "        ## hyperparameters\n",
        "        self.buffer_size = 2000\n",
        "        self.batch_size = 64\n",
        "        self.gamma = 0.99\n",
        "        self.lr = 0.0025\n",
        "        self.update_every = 4 \n",
        "\n",
        "        # Q-Network\n",
        "        self.local = DQNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.local.parameters(), lr=self.lr)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = deque(maxlen=self.buffer_size) \n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.t_step = 0\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.append(self.experience(state, action, reward, next_state, done)) \n",
        "        # Learn every update_every time steps.\n",
        "        self.t_step = (self.t_step + 1) % self.update_every\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > self.batch_size:\n",
        "                experiences = self.sample_experiences()\n",
        "                self.learn(experiences, self.gamma)\n",
        "    def act(self, state, eps=0.):\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "            self.local.eval()\n",
        "            with torch.no_grad():\n",
        "                action_values = self.local(state)\n",
        "            self.local.train()\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "    def learn(self, experiences, gamma): \n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "       # Get expected Q values from local model\n",
        "        Q_expected = self.local(states).gather(1, actions)\n",
        "\n",
        "        # Get max predicted Q values (for next states) from local model\n",
        "        Q_targets_next = self.local(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        # Compute Q targets for current states \n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        # Minimize the loss\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "    def sample_experiences(self):\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)        \n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)        \n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "agent = Agent(env.observation_space.shape[0], env.action_space.n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ov7oTYsYnowd"
      },
      "outputs": [],
      "source": [
        "scores = [] # list containing scores from each episode\n",
        "scores_window = deque(maxlen=100) # last 100 scores\n",
        "n_episodes=5000\n",
        "max_t=5000\n",
        "eps_start=1.0\n",
        "eps_end=0.001\n",
        "eps_decay=0.9995\n",
        "eps = eps_start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 893
        },
        "id": "mHOK_FWapNn7",
        "outputId": "9a7484a8-2a53-4647-ec5d-d4249ade4ed1"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m      3\u001b[0m state_size \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mobservation_space\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mreshape(state, [\u001b[39m1\u001b[39;49m, state_size])\n\u001b[1;32m      5\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_t):\n",
            "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:298\u001b[0m, in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_reshape_dispatcher)\n\u001b[1;32m    199\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreshape\u001b[39m(a, newshape, order\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mC\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    200\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m    Gives a new shape to an array without changing its data.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39m           [5, 6]])\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39mreshape\u001b[39;49m\u001b[39m'\u001b[39;49m, newshape, order\u001b[39m=\u001b[39;49morder)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
            "File \u001b[0;32m~/.local/lib/python3.8/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(asarray(obj), method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
            "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part."
          ]
        }
      ],
      "source": [
        "for i_episode in range(1, n_episodes+1):\n",
        "    state = env.reset()\n",
        "    state_size = env.observation_space.shape[0]\n",
        "    state = np.reshape(state, [1, state_size])\n",
        "    score = 0\n",
        "    for i in range(max_t):\n",
        "        action = agent.act(state, eps)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        next_state = np.reshape(next_state, [1, state_size])\n",
        "        reward = reward if not done or score == 499 else -10\n",
        "        agent.step(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        score += reward\n",
        "        if done:\n",
        "            break \n",
        "    scores_window.append(score) # save most recent score \n",
        "    scores.append(score) # save most recent score\n",
        "    eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
        "    print('\\rEpisode {}\\tReward {} \\tAverage Score: {:.2f} \\tEpsilon: {}'.format(i_episode,score,np.mean(scores_window), eps), end=\"\")\n",
        "    if i_episode % 100 == 0:\n",
        "        print('\\rEpisode {}\\tAverage Score: {:.2f} \\tEpsilon: {}'.format(i_episode, np.mean(scores_window), eps))\n",
        "    if i_episode>10 and np.mean(scores[-10:])>450:\n",
        "        break\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(scores)\n",
        "plt.title('Scores over increasing episodes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCoDl_OVpWhl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "Deep_Q_Learning_Cart_Pole_balancing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
