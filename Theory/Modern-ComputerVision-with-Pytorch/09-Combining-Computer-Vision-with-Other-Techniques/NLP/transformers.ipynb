{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The working details of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers have proven to be a remarkable architecture for sequence-to-sequence\n",
    "problems. Almost all NLP tasks, as of the time of writing this book, have state-of-the-\n",
    "art implementations that come from transformers. This class of networks uses only\n",
    "linear layers and softmax to create self-attention (which will be explained in detail in\n",
    "the next sub-section). Self-attention helps in identifying the interdependency among\n",
    "words in the input text. The input sequence typically does not exceed 2,048 items as\n",
    "this is large enough for text applications. However, if images are to be used with\n",
    "transformers, they have to be flattened, which creates a sequence in the order of\n",
    "thousands/millions of pixels (as a 300 x 300 x 3 image would contain 270K pixels),\n",
    "which is not feasible. Facebook Research came up with a novel way to bypass this\n",
    "restriction by giving the feature map (which has a smaller size than the input image)\n",
    "as input to the transformer. Let's understand the basics of transformers in this section\n",
    "and understand the relevant code blocks later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of a transformer is the self-attention module. It takes three two-\n",
    "dimensional matrices (called <b> query (Q) </b>, <b> key (K) </b>, and <b> value (V) </b> matrices) as input.\n",
    "The matrices can have very large embedding sizes (as they would contain text size x\n",
    "embedding size number of values), so they are split up into smaller components first\n",
    "(step 1 in the following diagram), before running through the scaled-dot-product-\n",
    "attention (step 2 in the following diagram)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's understand how self-attention works. In a hypothetical scenario where the\n",
    "sequence length is 3, we have three word embeddings ($W_1$ , $W_2$ , and $W_3$ ) as input. Say\n",
    "each embedding is of size 512. Each of these embeddings is individually converted\n",
    "into three additional vectors, which are the query, key, and value vectors\n",
    "corresponding to each input:\n",
    "\n",
    "![trans](../imgs/trans0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since each vector is 512 in size, it is computationally expensive to do a matrix\n",
    "multiplication between them. So, we split each of these vectors into eight parts,\n",
    "having eight sets of (64 x 3) vectors for each of key, query, and value tensor, where 64\n",
    "is obtained from 512 (embedding size) / 8 (multi-heads) and 3 is the sequence length:\n",
    "\n",
    "![trans](../imgs/trans1.png)\n",
    "\n",
    "Note tha there will be eight sets of tensors of $K_{w11}$, $K_{w12}$ and so on because there are eight multi-heads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each part, we first perform matrix multiplication between the key and query\n",
    "matrices. This way, we end up with a 3 x 3 matrix. Pass it through softmax activation.\n",
    "Now, we have a matrix showing how important each word is, in relation to every\n",
    "other word:\n",
    "\n",
    "![trans](../imgs/trans2.png)\n",
    "\n",
    "Finally, we perform matrix multiplication of the preceding tensor output with the\n",
    "value tensor to get the output of our self-attention operation:\n",
    "\n",
    "![trans](../imgs/trans3.png)\n",
    "\n",
    "We then combine the eight outputs of this step, go back using concat layer (step3 in\n",
    "the following diagram), and end up with a single tensor of size 512 x 3. Because of\n",
    "the splitting of the Q, K, and V matrices, the layer is also called <b> multi-head self-attention </b>\n",
    "\n",
    "![trans](../imgs/trans4.png)\n",
    "\n",
    "The idea behind such a complex-looking network is as follows:\n",
    "\n",
    "- <b> Values (Vs) </b> are the processed embeddings that need to be learned for a\n",
    "given input, in its context of key and query matrices.\n",
    "\n",
    "- <b> Queries (Qs) </b> and <b> Keys (Ks) </b> act in such a way that their combination will\n",
    "create the right mask so that only the important parts of the value matrix\n",
    "are fed to the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our example in computer vision, when searching for an object such as a horse, the\n",
    "query should contain information to search for an object that is large in dimension\n",
    "and is brown, black, or white in general. The softmax output of scaled dot-product\n",
    "attention will reflect those parts of the key matrix that contain this color (brown,\n",
    "black, white, and so on) in the image. Thus, the values output from the self-attention\n",
    "layer will have those parts of the image that are roughly of the desired color and are\n",
    "present in the values matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the self-attention block several times in the network, as illustrated in the\n",
    "following diagram. The transformer network contains an encoding network (the left\n",
    "part of the diagram) whose input is the source sequence. The output of the encoding\n",
    "half is used as the key and query inputs for the decoding half, while the value input is\n",
    "going to be learned by the neural network independently to the encoding half\n",
    "\n",
    "![trans](../imgs/trans5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, even though this is a sequence of inputs, there's no sign of which token\n",
    "(word) is first and which is next (since a linear layer has no positional indication).\n",
    "Positional encodings are learnable embeddings (and sometimes hardcoded vectors)\n",
    "that we add to each input as a function of its position in the sequence. This is done so\n",
    "that the network understands which word embedding is first in the sequence, which\n",
    "is second, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
