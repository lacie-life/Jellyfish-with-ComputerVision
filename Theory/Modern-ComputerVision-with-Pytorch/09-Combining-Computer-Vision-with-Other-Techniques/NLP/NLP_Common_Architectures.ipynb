{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introducing Recurrent Neural Networks (RNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An RNN can have multiple architectures. Some of the possible ways of architecting\n",
    "an RNN are as follows:\n",
    "\n",
    "![nlp_common_architectures](../imgs/nlp0.png)\n",
    "\n",
    "In the preceding diagram, the boxes at the bottom are the input, followed by the\n",
    "hidden layer (the middle boxes), and then the boxes at the top are the output layer.\n",
    "The one-to-one architecture is a typical neural network with a hidden layer between\n",
    "the input and output layers. Examples of different architectures are as follows:\n",
    "\n",
    "- One-to-many: The input is an image and the output is a caption of the\n",
    "image.\n",
    "\n",
    "- Many-to-one: The input is a movie review (multiple words in input) and\n",
    "the output is the sentiment associated with the review.\n",
    "\n",
    "- Many-to-many: Machine translation of a sentence in one language to a\n",
    "sentence in another language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The idea behind the need for RNN architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are useful when we want to predict the next event given a sequence of events.\n",
    "An example of that could be to predict the word that comes after this: <i>This is an ___</i>.\n",
    "\n",
    "Let's say that in reality, the sentence is <i>This is an example</i>.\n",
    "\n",
    "Traditional text-mining techniques would solve the problem in the following way:\n",
    "\n",
    "1. Encode each word while having an additional index for potential new\n",
    "words:\n",
    "\n",
    "<i>This</i> : {1, 0, 0, 0}\n",
    "\n",
    "<i>is</i> : {0, 1, 0, 0}\n",
    "\n",
    "<i>an</i> : {0, 0, 1, 0}\n",
    "\n",
    "2. Encode the phrase <i> This is an </i>:\n",
    "\n",
    "<i> This is an </i> : {1, 1, 1, 0}\n",
    "\n",
    "3. Create the traing dataset:\n",
    "\n",
    "Input --> {1, 1, 1, 0}\n",
    "\n",
    "Output --> {0, 0, 0, 1}\n",
    "\n",
    "4. Build a model with the given input and output combination:\n",
    "\n",
    "One of the major drawbacks of the model is that the input representation\n",
    "does not change in the input sentence regardless of if it is in the form of this\n",
    "is an, an is this, or this an is.\n",
    "\n",
    "However, intuitively, we know that each of the preceding sentences is\n",
    "different and cannot be represented by the same structure mathematically.\n",
    "This calls for having a different architecture, which looks as follows:\n",
    "\n",
    "![nlp](../imgs/nlp1.png)\n",
    "\n",
    "In the preceding architecture, each of the individual words from the sentence enters\n",
    "an individual box in the input boxes. This ensures that we preserve the structure of\n",
    "the input sentence; for example, this enters the first box, is enters the second box, and\n",
    "an enters the third box. The output box at the top will be the output – that is, example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the structure of an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of an RNN as a mechanism to hold memory – where the hidden layer\n",
    "contains the memory. The unfolded version of an RNN is as follows:\n",
    "\n",
    "![nlp](../imgs/nlp3.png)\n",
    "\n",
    "The network on right is an unrolled version of the network on the left. The network\n",
    "on the right takes one input in each time step and extracts the output at each time\n",
    "step.\n",
    "\n",
    "Note that while predicting the output of the third time step, we are incorporating\n",
    "values from the first two time steps through the hidden layer, which is connecting the\n",
    "values across time steps.\n",
    "\n",
    "Let's explore the preceding diagram:\n",
    "\n",
    "- The u weight represents the weights that connect the input layer to the\n",
    "hidden layer.\n",
    "\n",
    "- The w weight represents the hidden layer to the hidden layer connection.\n",
    "\n",
    "- The v weight represents the hidden layer to the output layer connection.\n",
    "\n",
    "The output in a given time step depends on both the input in the current time step\n",
    "and the hidden layer value from the previous time step. With the introduction of the\n",
    "hidden layer of the previous time step being the input, along with the current time\n",
    "step's input, we are obtaining information from the previous time steps. This way, we\n",
    "are creating a pipeline of connections that enable memory storage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why store memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a need to store memory as, in the preceding example, or even in text\n",
    "generation in general, the next word does not depend only on the preceding word,\n",
    "but also on the context of the words preceding the word to predict.\n",
    "\n",
    "Given that we are looking at the preceding words, there should be a way to keep\n",
    "them in memory so that we can predict the next word more accurately.\n",
    "\n",
    "We should also have the memory in order; more often than not, the recent words are\n",
    "more useful in predicting the next word than the words that are further away from\n",
    "the word to predict.\n",
    "\n",
    "A traditional RNN that takes multiple time steps into account for giving predictions\n",
    "can be visualized as follows:\n",
    "\n",
    "![nlp](../imgs/nlp4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that as the time step increases, the impact of the input present at a much\n",
    "earlier time step (time step 1) would be lower on the output at a much later time step\n",
    "(time step 7). An example of this can be seen here (for a moment, let's ignore the bias\n",
    "term and assume that the hidden layer input at time step 1 is 0 and we are predicting\n",
    "the value of the hidden layer at time step 5 – $h_5$ ):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlp](../imgs/nlp5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that as the time step increases, the value of the hidden layer (h 5 ) highly\n",
    "depends on $X_1$ if U>1; however, it is much less dependent on $X_1$ if U<1.\n",
    "\n",
    "The dependency on the U matrix can also result in the hidden layer ($h_5$) value being\n",
    "very small, hence resulting in a vanishing gradient when the value of U is very small,\n",
    "and can cause exploding gradients when the value of U is very high.\n",
    "\n",
    "The preceding phenomenon results in an issue when there is a long-term dependency\n",
    "on predicting the next word. To solve this problem, we'll use the LSTM architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory - LSTM architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we learned about how a traditional RNN faces a vanishing or\n",
    "exploding gradient problem resulting in it not being able to accommodate long-term\n",
    "memory. In this section, we will learn about how to leverage LSTM to get around this\n",
    "problem.\n",
    "\n",
    "In order to further understand the scenario with an example, let's consider the\n",
    "following sentence:\n",
    "\n",
    "<i> I am from England. I speak __. </i>\n",
    "\n",
    "In the preceding sentence, intuitively, we know that the majority of the people from\n",
    "England speak English. The blank value to be filled (English) is obtained from the fact\n",
    "that the person is from England. While in this scenario we have the signaling word\n",
    "(England) closer to the blank value, in a realistic scenario, we might find that the\n",
    "signal word is far away from the blank space (the word we are trying to predict).\n",
    "When the distance between the signal word and blank value is large, the predictions\n",
    "through traditional RNNs might be wrong because of the vanishing or exploding\n",
    "gradient phenomenon. LSTM addresses this scenario – which we will learn about in\n",
    "the following section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The working details of LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A standard LSTM architecture is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlp](../imgs/nlp6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding diagram, you can see that while input X and output h remain similar\n",
    "to what we saw in the Exploring the structure of an RNN section, the computations that\n",
    "happen between the input and output are different in LSTM. Let's understand the\n",
    "various activations that happen between the input and output:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![nlp](../imgs/nlp7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding diagram, we can observe the following:\n",
    "\n",
    "- $X$ and h represent the input and output at time step $t$.\n",
    "\n",
    "- $C$ represents the cell state. This potentially helps in storing long-term\n",
    "memory.\n",
    "\n",
    "- $C_{t-1}$ is the cell state that is transferred from the previous time step.\n",
    "\n",
    "- $h_{t-1}$ represents the output of the previous time step.\n",
    "\n",
    "- $f_t$ represents activations that help with forgetting certain information.\n",
    "\n",
    "- $i_t$ represents the transformation corresponding to the input combined with\n",
    "the previous time step's output ($h_{t-1}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content that needs to be forgotten, $f_t$ , is obtained as follows:\n",
    "\n",
    "![nlp](../imgs/nlp8.png)\n",
    "\n",
    "Note that $W_{xf}$ and $W_{hf}$ represent the weights associated with the input and the\n",
    "previous hidden layer, respectively.\n",
    "The cell state is updated by multiplying the cell state from the previous time step, $C_{t-1}$ ,\n",
    "by the input content that helps in forgetting: $f_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated cll state í á follows:\n",
    "\n",
    "![nlp](../imgs/nlp9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand how the preceding operations help, let's go through the input\n",
    "sentence: <i> I am from England. I speak __ </i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will include additional information from the current time step to\n",
    "the cell state as well as to the output. The modified cell state (after forgetting what is\n",
    "to be forgotten) is updated by the input activation (which is based on the current time\n",
    "step's input and also the previous time step's output) and the modulation\n",
    "gate, $g_t$ (which helps in identifying the amount by which the cell state is to be\n",
    "updated).\n",
    "\n",
    "The input activation is calculated as follows:\n",
    "\n",
    "![nlp](../imgs/nlp10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $W_{xi}$ and $W_{hi}$ represent the weights associated with the input and the\n",
    "previous hidden layer, respectively.\n",
    "\n",
    "The modified gate's activation is calculated as follows:\n",
    "\n",
    "![nlp](../imgs/nlp11.png)\n",
    "\n",
    "Note that $W_{xg}$ and $W_{hg}$ represent the weights associated with the input and the\n",
    "previous hidden layer, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The modified cell state, $C_t$ , which will be passed to the next time step, is now as\n",
    "follows:\n",
    "\n",
    "![nlp](../imgs/nlp12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we multiply the activated updated cell state ($tanh(C_t)$) by the activated output\n",
    "values, $O_t$ , to obtain the final output, $h_t$ , at time step $t$:\n",
    "\n",
    "![nlp](../imgs/nlp13.png)\n",
    "\n",
    "\n",
    "This way, we can leverage the various gates present in an LSTM to selectively\n",
    "memorize overly long time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda-pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
