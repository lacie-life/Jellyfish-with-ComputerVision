{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# Forward Propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uk4Fezbb9SZc"
      },
      "source": [
        "To build a strong foundational understanding of how feedforward propagation\n",
        "works, we'll go through a toy example of training a neural network where the input\n",
        "to the neural network is (1, 1) and the corresponding (expected) output is 0. Here, we\n",
        "are going to find the optimal weights of the neural network based on this single\n",
        "input-output pair. However, you should note that in reality, there will be thousands\n",
        "of data points on which an ANN is trained.\n",
        "\n",
        "The following figure shows the architecture of the neural network that we are going:\n",
        "\n",
        "![img](./imgs/ANN6.png)\n",
        "\n",
        "Every arrow in the preceding diagram contains exactly one float value (weight) that\n",
        "is adjustable. There are 9 (6 in the first hidden layer and 3 in the second) floats that we\n",
        "need to find, so that when the input is (1,1), the output is as close to (0) as possible.\n",
        "This is what we mean by training the neural network. We have not introduced a bias\n",
        "value yet, for simplicity purposes only â€“ the underlying logic remains the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating the hidden layer unit values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll now assign weights to all of the connections. In the first step, we assign weights\n",
        "randomly across all the connections. And in general, neural networks are\n",
        "initialized with random weights before the training starts. Again, for simplicity, while\n",
        "introducing the topic, we will not include the bias value while learning about\n",
        "feedforward propagation and backpropagation. But we will have it while\n",
        "implementing both feedforward propagation and backpropagation from scratch.\n",
        "\n",
        "Let's start with initial weights that are randomly initialized between 0 and 1, but note\n",
        "that the final weights after the training process of a neural network don't need to be\n",
        "between a specific set of values. A formal representation of weights and values in the\n",
        "network is provided in the following diagram (left half) and the randomly initialized\n",
        "weights are provided in the network in the right half.\n",
        "\n",
        "![img](./imgs/ANN7.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The hidden layer's unit values before activation are obtained as follows:\n",
        "\n",
        "![imgs](./imgs/ANN8.png)\n",
        "\n",
        "The hidden layer's unit values (before activation) that are calculated here are also\n",
        "shown in the following diagram:\n",
        "\n",
        "![imgs](./imgs/ANN9.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T12:46:03.594770Z",
          "start_time": "2020-09-24T12:46:03.589643Z"
        },
        "colab": {},
        "colab_type": "code",
        "id": "VytiqjTQgwf4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def feed_forward(inputs, outputs, weights):\n",
        "    # Matrix multiplication of inputs and weights       \n",
        "    pre_hidden = np.dot(inputs,weights[0])+ weights[1]\n",
        "    # Pass through activation function\n",
        "    hidden = 1/(1+np.exp(-pre_hidden))\n",
        "    # Output layer\n",
        "    pred_out = np.dot(hidden, weights[2]) + weights[3]\n",
        "    # Return error MSE\n",
        "    mean_squared_error = np.mean(np.square(pred_out - outputs))\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation function Example\n",
        "\n",
        "![imgs](./imgs/ann10.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.where(x>0, x, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating the output layer values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far, we have calculated the final hidden layer values after applying the sigmoid\n",
        "activation. Using the hidden layer values after activation, and the weight values\n",
        "(which are randomly initialized in the first iteration), we will calculate the output\n",
        "value for our network:\n",
        "\n",
        "![imgs](./imgs/ANN11.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We perform the sum of products of the hidden layer values and weight values to\n",
        "calculate the output value. Another reminder: we excluded the bias terms that need to\n",
        "be added at each unit(node), only to simplify our understanding of the working\n",
        "details of feedforward propagation and backpropagation for now and will include it\n",
        "while coding up feedforward propagation and backpropagation:\n",
        "\n",
        "![imgs](./imgs/ANN12.png)\n",
        "\n",
        "Because we started with a random set of weights, the value of the output node is very\n",
        "different from the target. In this case, the difference is 1.235 (remember, the target is\n",
        "0)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss Function\n",
        "\n",
        "### Calculating loss during continuous variable prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b> Mean squared error: </b> The mean squared error is the squared difference between the actual and the predicted values of the output. We take a square of the error, as the error can be positive or negative (when the predicted value is greater than the actual value and vice versa). Squaring ensures that positive and negative errors do not offset each other. We calculate the mean of the squared error so that the error over two different datasets is comparable when the datasets are not of the same size.\n",
        "\n",
        "The mean squared error is typically used when trying to predict a value that\n",
        "is continuous in nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse(p, y):\n",
        "    return np.mean((p-y)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Mean absolute error:</b> The mean absolute error works in a manner that is\n",
        "very similar to the mean squared error. The mean absolute error ensures\n",
        "that positive and negative errors do not offset each other by taking an\n",
        "average of the absolute difference between the actual and predicted values\n",
        "across all data points.\n",
        "\n",
        "Similar to the mean squared error, the mean absolute error is generally\n",
        "employed on continuous variables. Further, in general, it is preferable to\n",
        "have a mean absolute error as a loss function when the outputs to predict\n",
        "have a value less than 1, as the mean squared error would reduce the\n",
        "magnitude of loss considerably (the square of a number between 1 and -1 is\n",
        "an even smaller number) when the expected output is less than 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mae(p, y):\n",
        "    return np.mean(np.abs(p-y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Calculating loss during categorical variable prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Binary cross-entropy: </b>Cross-entropy is a measure of the difference between\n",
        "two different distributions: actual and predicted. Binary cross-entropy is\n",
        "applied to binary output data, unlike the previous two loss functions that\n",
        "we discussed (which are applied during continuous variable prediction).\n",
        "\n",
        "Note that binary cross-entropy loss has a high value when the predicted\n",
        "value is far away from the actual value and a low value when the predicted\n",
        "and actual values are close."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_cross_entropy(p, y):\n",
        "    return -np.mean((y * np.log(p) + (1 - y) * np.log(1 - p)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Categorical cross-entropy:</b> Categorical cross-entropy between an array of\n",
        "predicted values ( p ) and an array of actual values ( y ) is implemented as\n",
        "follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def categorical_cross_entropy(p, y):\n",
        "    return -np.mean(np.log(p[np.arange(len(y)), y]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Feed_forward_propagation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
