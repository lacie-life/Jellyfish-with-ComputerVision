{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Uk4Fezbb9SZc"
      },
      "source": [
        "### Forward Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-24T12:46:03.594770Z",
          "start_time": "2020-09-24T12:46:03.589643Z"
        },
        "colab": {},
        "colab_type": "code",
        "id": "VytiqjTQgwf4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "def feed_forward(inputs, outputs, weights):\n",
        "    # Matrix multiplication of inputs and weights       \n",
        "    pre_hidden = np.dot(inputs,weights[0])+ weights[1]\n",
        "    # Pass through activation function\n",
        "    hidden = 1/(1+np.exp(-pre_hidden))\n",
        "    # Output layer\n",
        "    pred_out = np.dot(hidden, weights[2]) + weights[3]\n",
        "    # Return error MSE\n",
        "    mean_squared_error = np.mean(np.square(pred_out - outputs))\n",
        "    return mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Activation function Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return np.where(x>0, x, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b> Mean squared error: </b> The mean squared error is the squared difference between the actual and the predicted values of the output. We take a square of the error, as the error can be positive or negative (when the predicted value is greater than the actual value and vice versa). Squaring ensures that positive and negative errors do not offset each other. We calculate the mean of the squared error so that the error over two different datasets is comparable when the datasets are not of the same size.\n",
        "\n",
        "The mean squared error is typically used when trying to predict a value that\n",
        "is continuous in nature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mse(p, y):\n",
        "    return np.mean((p-y)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Mean absolute error:</b> The mean absolute error works in a manner that is\n",
        "very similar to the mean squared error. The mean absolute error ensures\n",
        "that positive and negative errors do not offset each other by taking an\n",
        "average of the absolute difference between the actual and predicted values\n",
        "across all data points.\n",
        "\n",
        "Similar to the mean squared error, the mean absolute error is generally\n",
        "employed on continuous variables. Further, in general, it is preferable to\n",
        "have a mean absolute error as a loss function when the outputs to predict\n",
        "have a value less than 1, as the mean squared error would reduce the\n",
        "magnitude of loss considerably (the square of a number between 1 and -1 is\n",
        "an even smaller number) when the expected output is less than 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mae(p, y):\n",
        "    return np.mean(np.abs(p-y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Binary cross-entropy: </b>Cross-entropy is a measure of the difference between\n",
        "two different distributions: actual and predicted. Binary cross-entropy is\n",
        "applied to binary output data, unlike the previous two loss functions that\n",
        "we discussed (which are applied during continuous variable prediction).\n",
        "\n",
        "Note that binary cross-entropy loss has a high value when the predicted\n",
        "value is far away from the actual value and a low value when the predicted\n",
        "and actual values are close."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def binary_cross_entropy(p, y):\n",
        "    return -np.mean((y * np.log(p) + (1 - y) * np.log(1 - p)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Categorical cross-entropy:</b> Categorical cross-entropy between an array of\n",
        "predicted values ( p ) and an array of actual values ( y ) is implemented as\n",
        "follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def categorical_cross_entropy(p, y):\n",
        "    return -np.mean(np.log(p[np.arange(len(y)), y]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Feed_forward_propagation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
